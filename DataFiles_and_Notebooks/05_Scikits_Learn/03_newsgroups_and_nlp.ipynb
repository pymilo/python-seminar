{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Text (e.g., newsgroups)\n",
    "\n",
    "We'll step through a reasonably complete ML workflow and test the accuracy of a few of the ML algorithms we've discussed in class on the [20 newsgroups]( http://qwone.com/~jason/20Newsgroups) data set using [nltk](http://nltk.org/) and [scikit-learn](http://scikit-learn.org/).\n",
    "\n",
    "By default, nltk only includes a small sample of the 20 newsgroups data, so for this demo you'll need to download the complete collection of texts, from [here](http://people.csail.mit.edu/jrennie/20Newsgroups/20news-18828.tar.gz).\n",
    "\n",
    "Shamelessly adapted from:\n",
    "http://nbviewer.ipython.org/urls/dl.dropboxusercontent.com/u/4864294/ML/20%2520newsgroups.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "Once you've downloaded and uncompressed the collection, you should have a folder called '20news-18828' in your current directory.  We can now use set up nltk's corpus tools to allow us to easily access the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "newsgroups = \\\n",
    "  nltk.corpus.PlaintextCorpusReader('/Users/jbloom/Downloads/20news-18828', '.*/[0-9]+', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An nltk corpus can be viewed as a collection of files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism/49960',\n",
       " 'comp.sys.mac.hardware/51628',\n",
       " 'rec.motorcycles/104509',\n",
       " 'sci.electronics/53771',\n",
       " 'talk.politics.guns/54514']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = newsgroups.fileids()\n",
    "print(len(ids))\n",
    "ids[::4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: levy@levy.fnal.gov (Mark E. Levy, ext. 8056)\r\n",
      "Subject: Sources for Intel D87C51FB?\r\n",
      "\r\n",
      "I am in the midst of designing a project which requires two motors and an LED\r\n",
      "illuminator driven with Pulse-width modulation.  I'm using the 8751, and\r\n",
      "I understand that the -FB version has a programmable counter array that can\r\n",
      "essentially be set and forgotten to do the PWM.  The problems is, that variant\r\n",
      "is difficult to come by.  I need two or three of the D prefix (ceramic window)\r\n",
      "version for development, and then lots of the P prefix (plastic OTP) for later\r\n",
      "production.  I've tried Avnet, Arrow, and Pioneer.  They (might) have them, but\r\n",
      "I'm looking for samples at this point, and they're not too willing to provide\r\n",
      "them.  I would buy them, but these vendors have $100.00 minimums.\r\n",
      "\r\n",
      "Any help is appreciated.\r\n",
      "\r\n",
      "================================================================================\r\n",
      "[ Mark E. Levy, Fermilab          |                                            ]\r\n",
      "[ BitNet:   LEVY@FNAL             | Unix is to computing                       ]\r\n",
      "[ Internet: LEVY@FNALD.FNAL.GOV   |       as an Etch-a-Sketch is to art.       ]\r\n",
      "[ HEPnet/SPAN: FNALD::LEVY (VMS!) |                                            ]\r\n",
      "================================================================================\r\n"
     ]
    }
   ],
   "source": [
    "!cat /Users/jbloom/Downloads/20news-18828/sci.electronics/53771"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the list of file ids, randomly shuffle it, and divide it into training and test sections.  And, to make the demo go faster, we'll only use a small sample of the available text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500 500\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "random.shuffle(ids)\n",
    "ids = ids[:5000]\n",
    "size = len(ids)\n",
    "\n",
    "testSet = ids[:int(size*0.1)]\n",
    "trainSet = ids[int(size*0.1):]\n",
    "\n",
    "print(len(trainSet), len(testSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "\n",
    "Now that we've got our data, we need to convert our newsgroup texts into features that can be used by machine learning algorithms.  For this example, we'll use lexical features: each word is a feature, and it's value is the number of times that word occurs in a text. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'unintentional': 1, 'haganah': 1, 'since': 1, 'odd': 1, 'down': 1, 'seen': 1, 'testimony': 2, 'stupidly': 1, 'given': 2, 'be': 1, 'extremists': 1, 'whom': 1, 'pale': 1, 'pictures': 2, 'didn': 1, 'him': 1, 'people': 1, 'photographer': 1, 'soldier': 1, 'see': 2, 'any': 2, 'nothing': 2, 'dir': 3, 'if': 2, 'deposited': 1, 'ca': 2, 'ere': 2, 'remember': 1, 'is': 4, 'historian': 1, 't': 4, 'hassan': 1, 'martinb': 1, 'askew': 3, 'no': 10, 'you': 7, 'remote': 1, 'hundreds': 1, 'before': 1, 'hands': 1, 'why': 1, 'sound': 1, 'proud': 1, 'had': 3, 'to': 17, 'surviving': 1, 'credible': 1, 'writes': 2, 'somehow': 1, 'really': 2, 'on': 3, 'head': 2, 'likud': 1, 'a': 12, 'have': 3, 'own': 3, 'want': 3, 'which': 1, 'for': 6, 'absolutely': 1, 'stern': 1, 'has': 1, 'brise': 1, 'van': 1, 'brown': 2, 'brenda': 1, 'adelaide': 2, 'government': 3, 'gotten': 1, 'sue': 1, 'deir': 1, 'grief': 1, 'arab': 4, 'israeli': 1, 'exactly': 1, 'else': 3, 'source': 2, 'ethnic': 1, 'now': 1, 'significance': 2, 'or': 2, 'because': 1, 'pleiades': 1, 'report': 3, 'yes': 4, 'behind': 1, 'seems': 1, 'they': 6, 'slept': 1, 'there': 3, 'begin': 2, 'snipers': 1, 'river': 1, 'suppressed': 1, 'cleansing': 1, 'even': 2, 'secondly': 1, 'all': 5, 'name': 2, 'simple': 1, 'irgun': 2, 'in': 13, 'autumn': 1, 'out': 1, 'sent': 2, 'reason': 3, 'mamaysky': 1, 'course': 2, 'like': 2, 'went': 1, 'died': 1, 'either': 1, 'children': 1, 'housed': 2, 'not': 9, 'happened': 1, 'disclaimer': 1, 'we': 1, 'fighting': 2, 'iraqi': 1, 'of': 14, 'so': 5, 'revisionists': 1, 'rather': 2, 'joseph': 3, 'might': 1, 'survived': 1, 'thorny': 1, 'occupants': 1, 'article': 2, 'ditch': 1, 'fighters': 1, 'die': 1, 'was': 16, 'effect': 1, 'fell': 1, 'many': 2, 'least': 1, 'noone': 1, 'mass': 2, 'attacked': 3, 'end': 1, 'who': 5, 'will': 1, 'local': 1, 'inhabitants': 2, 'yeah': 1, 'angell': 1, 'posters': 1, 'slander': 1, 'quarry': 1, 'that': 4, 'arabs': 1, 'soldiers': 1, 'clearly': 2, 'thus': 1, 'brought': 1, 'somwhere': 1, 'at': 3, 'taken': 1, 'this': 8, 'their': 7, 'anyone': 1, 'tied': 1, 'got': 1, 'its': 3, 'leave': 1, 'clear': 1, 'involved': 1, 'with': 4, 'and': 12, 'premeditated': 2, 'he': 2, 'actually': 1, 'decency': 1, 'power': 1, 'murder': 3, 'into': 1, 're': 1, 'archives': 2, 'edu': 4, 'claiming': 1, 'yassin': 4, 'massacre': 4, 'labour': 1, 'men': 5, 'cs': 2, 'compare': 1, 'resolve': 1, 'by': 2, 'sky': 1, 'claim': 3, 'maths': 2, 'armed': 1, 'way': 1, 'military': 2, 'are': 2, 'then': 3, 'only': 1, 'perhaps': 1, 'first': 1, 'aren': 1, 'take': 1, 'attack': 1, 'unarmed': 1, 'most': 1, 'chance': 1, 'troops': 3, 'fascists': 1, 'but': 2, 'gauche': 1, 'fact': 1, 'subject': 1, 'am': 1, 'release': 1, 'backs': 1, 'beyond': 1, 'standards': 1, 'carried': 1, 'the': 42, 'agree': 1, 'women': 2, 'au': 1, 'from': 1, 'some': 3, 'part': 1, 'responsible': 1, 'admitted': 1, 'aurag': 2, 'umontreal': 2, 'as': 4, 'killed': 1, 'warning': 2, 'almost': 1, 'replying': 1, 'bogged': 1, 'intention': 2, 'hm': 2, 'civilians': 5, 'were': 2, 'one': 1, 'must': 1, 'it': 14, 'deserts': 1, 'anymore': 1, 'shot': 3, 'rational': 1, 'them': 1, 'tents': 1, 'jaskew': 2, 'harry': 1, 'spam': 2, 'wierd': 1, 'back': 1, 'did': 6, 'doubt': 1, 'just': 1, 'intentions': 2, 'thye': 1, 'believe': 1, 'haven': 1, 'yet': 2, 'killing': 4, 'facts': 1, 'tell': 1, 'hostile': 1, 'would': 1, 'right': 1, 'north': 1, 'intentional': 1, 'except': 1, 'holocaust': 1, 'absurd': 1, 'murmurs': 1, 'other': 1, 'asserts': 1, 'stillness': 1, 'i': 6, 'me': 1, 'said': 3, 'kill': 3, 'village': 5, 'left': 1, 'don': 1, 'care': 1, 'work': 1, 'our': 1, 'do': 1, 'few': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def features(text):\n",
    "    \"\"\"Convert a post into a dictionary of features\"\"\"\n",
    "    features = defaultdict(int)\n",
    "    for word in text:\n",
    "        if word.isalpha():\n",
    "            features[word.lower()] += 1\n",
    "    return features\n",
    "\n",
    "print(features(newsgroups.words(fileids=trainSet[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also extract the class names for training instances.  For the class, we'll just use the name of the newsgroup that the text was taken from, and we can get that from the first part of the fileid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.politics.mideast\n"
     ]
    }
   ],
   "source": [
    "def getclass(fileid):\n",
    "    \"\"\"Get class name from fileid\"\"\"\n",
    "    return fileid.split('/')[0]\n",
    "\n",
    "print(getclass(trainSet[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll apply these functions to all the posts in the dataset (this may a while!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.74 s, sys: 1.43 s, total: 7.17 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%time trainData = [(features(newsgroups.words(fileids=f)),getclass(f)) for f in trainSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 732 ms, sys: 176 ms, total: 908 ms\n",
      "Wall time: 1.88 s\n"
     ]
    }
   ],
   "source": [
    "%time testData = [(features(newsgroups.words(fileids=f)),getclass(f)) for f in testSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(int,\n",
       "             {'a': 12,\n",
       "              'absolutely': 1,\n",
       "              'absurd': 1,\n",
       "              'actually': 1,\n",
       "              'adelaide': 2,\n",
       "              'admitted': 1,\n",
       "              'agree': 1,\n",
       "              'all': 5,\n",
       "              'almost': 1,\n",
       "              'am': 1,\n",
       "              'and': 12,\n",
       "              'angell': 1,\n",
       "              'any': 2,\n",
       "              'anymore': 1,\n",
       "              'anyone': 1,\n",
       "              'arab': 4,\n",
       "              'arabs': 1,\n",
       "              'archives': 2,\n",
       "              'are': 2,\n",
       "              'aren': 1,\n",
       "              'armed': 1,\n",
       "              'article': 2,\n",
       "              'as': 4,\n",
       "              'askew': 3,\n",
       "              'asserts': 1,\n",
       "              'at': 3,\n",
       "              'attack': 1,\n",
       "              'attacked': 3,\n",
       "              'au': 1,\n",
       "              'aurag': 2,\n",
       "              'autumn': 1,\n",
       "              'back': 1,\n",
       "              'backs': 1,\n",
       "              'be': 1,\n",
       "              'because': 1,\n",
       "              'before': 1,\n",
       "              'begin': 2,\n",
       "              'behind': 1,\n",
       "              'believe': 1,\n",
       "              'beyond': 1,\n",
       "              'bogged': 1,\n",
       "              'brenda': 1,\n",
       "              'brise': 1,\n",
       "              'brought': 1,\n",
       "              'brown': 2,\n",
       "              'but': 2,\n",
       "              'by': 2,\n",
       "              'ca': 2,\n",
       "              'care': 1,\n",
       "              'carried': 1,\n",
       "              'chance': 1,\n",
       "              'children': 1,\n",
       "              'civilians': 5,\n",
       "              'claim': 3,\n",
       "              'claiming': 1,\n",
       "              'cleansing': 1,\n",
       "              'clear': 1,\n",
       "              'clearly': 2,\n",
       "              'compare': 1,\n",
       "              'course': 2,\n",
       "              'credible': 1,\n",
       "              'cs': 2,\n",
       "              'decency': 1,\n",
       "              'deir': 1,\n",
       "              'deposited': 1,\n",
       "              'deserts': 1,\n",
       "              'did': 6,\n",
       "              'didn': 1,\n",
       "              'die': 1,\n",
       "              'died': 1,\n",
       "              'dir': 3,\n",
       "              'disclaimer': 1,\n",
       "              'ditch': 1,\n",
       "              'do': 1,\n",
       "              'don': 1,\n",
       "              'doubt': 1,\n",
       "              'down': 1,\n",
       "              'edu': 4,\n",
       "              'effect': 1,\n",
       "              'either': 1,\n",
       "              'else': 3,\n",
       "              'end': 1,\n",
       "              'ere': 2,\n",
       "              'ethnic': 1,\n",
       "              'even': 2,\n",
       "              'exactly': 1,\n",
       "              'except': 1,\n",
       "              'extremists': 1,\n",
       "              'fact': 1,\n",
       "              'facts': 1,\n",
       "              'fascists': 1,\n",
       "              'fell': 1,\n",
       "              'few': 1,\n",
       "              'fighters': 1,\n",
       "              'fighting': 2,\n",
       "              'first': 1,\n",
       "              'for': 6,\n",
       "              'from': 1,\n",
       "              'gauche': 1,\n",
       "              'given': 2,\n",
       "              'got': 1,\n",
       "              'gotten': 1,\n",
       "              'government': 3,\n",
       "              'grief': 1,\n",
       "              'had': 3,\n",
       "              'haganah': 1,\n",
       "              'hands': 1,\n",
       "              'happened': 1,\n",
       "              'harry': 1,\n",
       "              'has': 1,\n",
       "              'hassan': 1,\n",
       "              'have': 3,\n",
       "              'haven': 1,\n",
       "              'he': 2,\n",
       "              'head': 2,\n",
       "              'him': 1,\n",
       "              'historian': 1,\n",
       "              'hm': 2,\n",
       "              'holocaust': 1,\n",
       "              'hostile': 1,\n",
       "              'housed': 2,\n",
       "              'hundreds': 1,\n",
       "              'i': 6,\n",
       "              'if': 2,\n",
       "              'in': 13,\n",
       "              'inhabitants': 2,\n",
       "              'intention': 2,\n",
       "              'intentional': 1,\n",
       "              'intentions': 2,\n",
       "              'into': 1,\n",
       "              'involved': 1,\n",
       "              'iraqi': 1,\n",
       "              'irgun': 2,\n",
       "              'is': 4,\n",
       "              'israeli': 1,\n",
       "              'it': 14,\n",
       "              'its': 3,\n",
       "              'jaskew': 2,\n",
       "              'joseph': 3,\n",
       "              'just': 1,\n",
       "              'kill': 3,\n",
       "              'killed': 1,\n",
       "              'killing': 4,\n",
       "              'labour': 1,\n",
       "              'least': 1,\n",
       "              'leave': 1,\n",
       "              'left': 1,\n",
       "              'like': 2,\n",
       "              'likud': 1,\n",
       "              'local': 1,\n",
       "              'mamaysky': 1,\n",
       "              'many': 2,\n",
       "              'martinb': 1,\n",
       "              'mass': 2,\n",
       "              'massacre': 4,\n",
       "              'maths': 2,\n",
       "              'me': 1,\n",
       "              'men': 5,\n",
       "              'might': 1,\n",
       "              'military': 2,\n",
       "              'most': 1,\n",
       "              'murder': 3,\n",
       "              'murmurs': 1,\n",
       "              'must': 1,\n",
       "              'name': 2,\n",
       "              'no': 10,\n",
       "              'noone': 1,\n",
       "              'north': 1,\n",
       "              'not': 9,\n",
       "              'nothing': 2,\n",
       "              'now': 1,\n",
       "              'occupants': 1,\n",
       "              'odd': 1,\n",
       "              'of': 14,\n",
       "              'on': 3,\n",
       "              'one': 1,\n",
       "              'only': 1,\n",
       "              'or': 2,\n",
       "              'other': 1,\n",
       "              'our': 1,\n",
       "              'out': 1,\n",
       "              'own': 3,\n",
       "              'pale': 1,\n",
       "              'part': 1,\n",
       "              'people': 1,\n",
       "              'perhaps': 1,\n",
       "              'photographer': 1,\n",
       "              'pictures': 2,\n",
       "              'pleiades': 1,\n",
       "              'posters': 1,\n",
       "              'power': 1,\n",
       "              'premeditated': 2,\n",
       "              'proud': 1,\n",
       "              'quarry': 1,\n",
       "              'rather': 2,\n",
       "              'rational': 1,\n",
       "              're': 1,\n",
       "              'really': 2,\n",
       "              'reason': 3,\n",
       "              'release': 1,\n",
       "              'remember': 1,\n",
       "              'remote': 1,\n",
       "              'replying': 1,\n",
       "              'report': 3,\n",
       "              'resolve': 1,\n",
       "              'responsible': 1,\n",
       "              'revisionists': 1,\n",
       "              'right': 1,\n",
       "              'river': 1,\n",
       "              'said': 3,\n",
       "              'secondly': 1,\n",
       "              'see': 2,\n",
       "              'seems': 1,\n",
       "              'seen': 1,\n",
       "              'sent': 2,\n",
       "              'shot': 3,\n",
       "              'significance': 2,\n",
       "              'simple': 1,\n",
       "              'since': 1,\n",
       "              'sky': 1,\n",
       "              'slander': 1,\n",
       "              'slept': 1,\n",
       "              'snipers': 1,\n",
       "              'so': 5,\n",
       "              'soldier': 1,\n",
       "              'soldiers': 1,\n",
       "              'some': 3,\n",
       "              'somehow': 1,\n",
       "              'somwhere': 1,\n",
       "              'sound': 1,\n",
       "              'source': 2,\n",
       "              'spam': 2,\n",
       "              'standards': 1,\n",
       "              'stern': 1,\n",
       "              'stillness': 1,\n",
       "              'stupidly': 1,\n",
       "              'subject': 1,\n",
       "              'sue': 1,\n",
       "              'suppressed': 1,\n",
       "              'survived': 1,\n",
       "              'surviving': 1,\n",
       "              't': 4,\n",
       "              'take': 1,\n",
       "              'taken': 1,\n",
       "              'tell': 1,\n",
       "              'tents': 1,\n",
       "              'testimony': 2,\n",
       "              'that': 4,\n",
       "              'the': 42,\n",
       "              'their': 7,\n",
       "              'them': 1,\n",
       "              'then': 3,\n",
       "              'there': 3,\n",
       "              'they': 6,\n",
       "              'this': 8,\n",
       "              'thorny': 1,\n",
       "              'thus': 1,\n",
       "              'thye': 1,\n",
       "              'tied': 1,\n",
       "              'to': 17,\n",
       "              'troops': 3,\n",
       "              'umontreal': 2,\n",
       "              'unarmed': 1,\n",
       "              'unintentional': 1,\n",
       "              'van': 1,\n",
       "              'village': 5,\n",
       "              'want': 3,\n",
       "              'warning': 2,\n",
       "              'was': 16,\n",
       "              'way': 1,\n",
       "              'we': 1,\n",
       "              'went': 1,\n",
       "              'were': 2,\n",
       "              'which': 1,\n",
       "              'who': 5,\n",
       "              'whom': 1,\n",
       "              'why': 1,\n",
       "              'wierd': 1,\n",
       "              'will': 1,\n",
       "              'with': 4,\n",
       "              'women': 2,\n",
       "              'work': 1,\n",
       "              'would': 1,\n",
       "              'writes': 2,\n",
       "              'yassin': 4,\n",
       "              'yeah': 1,\n",
       "              'yes': 4,\n",
       "              'yet': 2,\n",
       "              'you': 7}),\n",
       " 'talk.politics.mideast')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Now that we've got all our posts converted into features and classes, we can try building some classifiers.  First, we'll establish a baseline score: how accurate is a classifier that assigns the most frequent class to every instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c383a41caef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "list(c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 20 samples and 4500 outcomes>\n"
     ]
    }
   ],
   "source": [
    "c = nltk.FreqDist(item[1] for item in trainData)\n",
    "default = list(c.keys())[0]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.068"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(c==default for f,c in testData) / float(len(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Next, we can try a few variations on the Naive Bayes classifier.  Nltk includes a simple function for training Naive Bayes classifiers which is very easy to use, though slow and not very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = nltk.NaiveBayesClassifier.train(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk also provides easy tools for evaluating classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.654\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(nb, testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk also has functions that allow us to call other machine learning libraries, including scikit-learn, using wrapper classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.496\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bernoulli = SklearnClassifier(BernoulliNB())\n",
    "bernoulli.train(trainData)\n",
    "\n",
    "print(nltk.classify.accuracy(bernoulli, testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multi = SklearnClassifier(MultinomialNB())\n",
    "multi.train(trainData)\n",
    "\n",
    "print(nltk.classify.accuracy(multi, testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn includes functions for performing feature selection and error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('chi2', SelectKBest(chi2, k=1000)),\n",
    "                     ('nb', MultinomialNB())])\n",
    "\n",
    "pmulti = SklearnClassifier(pipeline)\n",
    "pmulti.train(trainData)\n",
    "\n",
    "print(nltk.classify.accuracy(pmulti, testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "We can use any of the learning algorithms implemented by scikit-learn ([decision trees](http://scikit-learn.org/stable/modules/tree.html#classification), [knn](http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification), maxent [aka [logistic regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)], [adaboost](http://scikit-learn.org/stable/modules/ensemble.html#adaboost), [linear](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn-svm-linearsvc) and [non-linear](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn-svm-svc) SVMs, etc) in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = SklearnClassifier(LinearSVC())\n",
    "svm.train(trainData)\n",
    "\n",
    "print(nltk.classify.accuracy(svm, testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('comp.windows.x', 'comp.windows.x')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = svm.classify_many(item[0] for item in testData)\n",
    "results[0], testData[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0  0  0]\n",
      " [ 1 16  0  1  2  0  0  0  0  0  0  0  3  1  0  0  0  0  0  0]\n",
      " [ 0  1 22  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  1  0]\n",
      " [ 0  1  2 18  1  2  0  1  0  0  0  1  0  2  0  0  0  0  0  0]\n",
      " [ 0  2  1  1 16  0  0  0  1  0  0  0  2  1  0  0  0  1  0  0]\n",
      " [ 0  3  1  2  3 16  0  0  0  0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 13  0  1  0  1  1  0  0  1  0  0  0  0  0]\n",
      " [ 0  2  0  1  0  0  0 15  0  0  0  0  0  1  0  0  0  0  1  1]\n",
      " [ 0  1  0  0  0  0  0  0 30  0  0  0  0  1  0  1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  1 28  0  0  1  0  0  1  0  0  0  0]\n",
      " [ 0  1  0  1  0  0  0  0  0  2 22  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  1  0 20  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  1  1  2  1  1  2  0  0  0  0 25  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  1  1  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  1  0  0  0  0 26  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  0 19  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  0 19  0  2  1]\n",
      " [ 0  1  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0 21  2  1]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  3  1 21  0]\n",
      " [ 2  0  1  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  9]]\n",
      "(20, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x12015a0f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ0AAAEZCAYAAACNVXCFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZln+HzpHt8RN2C5mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f0cfb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cmm = confusion_matrix([x[1] for x in testData], results)\n",
    "\n",
    "print(cmm)\n",
    "cmm = np.array(cmm,dtype=np.float)\n",
    "print(cmm.shape)\n",
    "\n",
    "f,ax = plt.subplots()\n",
    "\n",
    "# Show confusion matrix in a separate window\n",
    "ax.imshow(cmm,interpolation='nearest')\n",
    "ax.set_title('Confusion matrix')\n",
    "ax.set_ylabel('True label')\n",
    "ax.set_xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
