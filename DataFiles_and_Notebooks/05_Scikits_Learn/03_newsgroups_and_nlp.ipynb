{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Text (e.g., newsgroups)\n",
    "\n",
    "We'll step through a reasonably complete ML workflow and test the accuracy of a few of the ML algorithms we've discussed in class on the [20 newsgroups]( http://qwone.com/~jason/20Newsgroups) data set using [nltk](http://nltk.org/) and [scikit-learn](http://scikit-learn.org/).\n",
    "\n",
    "By default, nltk only includes a small sample of the 20 newsgroups data, so for this demo you'll need to download the complete collection of texts, from [here](http://people.csail.mit.edu/jrennie/20Newsgroups/20news-18828.tar.gz).\n",
    "\n",
    "Shamelessly adapted from:\n",
    "http://nbviewer.ipython.org/urls/dl.dropboxusercontent.com/u/4864294/ML/20%2520newsgroups.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "Once you've downloaded and uncompressed the collection, you should have a folder called '20news-18828' in your current directory.  We can now use set up nltk's corpus tools to allow us to easily access the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "newsgroups = \\\n",
    "  nltk.corpus.PlaintextCorpusReader('/Users/jbloom/Downloads/20news-18828', '.*/[0-9]+', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An nltk corpus can be viewed as a collection of files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = newsgroups.fileids()\n",
    "print(len(ids))\n",
    "ids[::4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /Users/jbloom/Downloads/20news-18828/sci.electronics/53771"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the list of file ids, randomly shuffle it, and divide it into training and test sections.  And, to make the demo go faster, we'll only use a small sample of the available text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "random.shuffle(ids)\n",
    "ids = ids[:5000]\n",
    "size = len(ids)\n",
    "\n",
    "testSet = ids[:int(size*0.1)]\n",
    "trainSet = ids[int(size*0.1):]\n",
    "\n",
    "print(len(trainSet), len(testSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "\n",
    "Now that we've got our data, we need to convert our newsgroup texts into features that can be used by machine learning algorithms.  For this example, we'll use lexical features: each word is a feature, and it's value is the number of times that word occurs in a text. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def features(text):\n",
    "    \"\"\"Convert a post into a dictionary of features\"\"\"\n",
    "    features = defaultdict(int)\n",
    "    for word in text:\n",
    "        if word.isalpha():\n",
    "            features[word.lower()] += 1\n",
    "    return features\n",
    "\n",
    "print(features(newsgroups.words(fileids=trainSet[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also extract the class names for training instances.  For the class, we'll just use the name of the newsgroup that the text was taken from, and we can get that from the first part of the fileid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getclass(fileid):\n",
    "    \"\"\"Get class name from fileid\"\"\"\n",
    "    return fileid.split('/')[0]\n",
    "\n",
    "print(getclass(trainSet[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll apply these functions to all the posts in the dataset (this may a while!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time trainData = [(features(newsgroups.words(fileids=f)),getclass(f)) for f in trainSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time testData = [(features(newsgroups.words(fileids=f)),getclass(f)) for f in testSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainData[0][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Now that we've got all our posts converted into features and classes, we can try building some classifiers.  First, we'll establish a baseline score: how accurate is a classifier that assigns the most frequent class to every instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nltk.FreqDist(item[1] for item in trainData)\n",
    "default = list(c.keys())[0]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(c==default for f,c in testData) / float(len(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Next, we can try a few variations on the Naive Bayes classifier.  Nltk includes a simple function for training Naive Bayes classifiers which is very easy to use, though slow and not very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = nltk.NaiveBayesClassifier.train(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk also provides easy tools for evaluating classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(nb, testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk also has functions that allow us to call other machine learning libraries, including scikit-learn, using wrapper classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bernoulli = SklearnClassifier(BernoulliNB())\n",
    "bernoulli.train(trainData)\n",
    "\n",
    "print(nltk.classify.accuracy(bernoulli, testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multi = SklearnClassifier(MultinomialNB())\n",
    "multi.train(trainData)\n",
    "\n",
    "print(nltk.classify.accuracy(multi, testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn includes functions for performing feature selection and error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('chi2', SelectKBest(chi2, k=1000)),\n",
    "                     ('nb', MultinomialNB())])\n",
    "\n",
    "pmulti = SklearnClassifier(pipeline)\n",
    "pmulti.train(trainData)\n",
    "\n",
    "print(nltk.classify.accuracy(pmulti, testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "We can use any of the learning algorithms implemented by scikit-learn ([decision trees](http://scikit-learn.org/stable/modules/tree.html#classification), [knn](http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification), maxent [aka [logistic regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)], [adaboost](http://scikit-learn.org/stable/modules/ensemble.html#adaboost), [linear](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn-svm-linearsvc) and [non-linear](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn-svm-svc) SVMs, etc) in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = SklearnClassifier(LinearSVC())\n",
    "svm.train(trainData)\n",
    "\n",
    "print(nltk.classify.accuracy(svm, testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = svm.classify_many(item[0] for item in testData)\n",
    "results[0], testData[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cmm = confusion_matrix([x[1] for x in testData], results)\n",
    "\n",
    "print(cmm)\n",
    "cmm = np.array(cmm,dtype=np.float)\n",
    "print(cmm.shape)\n",
    "\n",
    "f,ax = plt.subplots()\n",
    "\n",
    "# Show confusion matrix in a separate window\n",
    "ax.imshow(cmm,interpolation='nearest')\n",
    "ax.set_title('Confusion matrix')\n",
    "ax.set_ylabel('True label')\n",
    "ax.set_xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Deep learning approach to understanding word meeting (in the context of sentences), and by extension paragraphs and documents. The main module for this is gensim. See https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "- Talk on word2vec https://www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994\n",
    "\n",
    "- https://www.kernix.com/blog/similarity-measure-of-textual-documents_p12\n",
    "- https://github.com/sdimi/average-word2vec/blob/master/notebook.ipynb\n",
    "- Doc2vec on newsgroups: https://github.com/skillachie/nlpArea51/tree/master/doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
