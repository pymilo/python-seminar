{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ../00_AdvancedPythonConcepts/talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression ## \n",
    "Let's take a look at the famous Boston Housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "boston = datasets.load_boston() # Boston house-prices\n",
    "X = boston['data']   # 13 features (e.g. crime, # rooms, age, etc.)\n",
    "Y = boston['target'] # response (median house price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"feature vector shape=\", X.shape)\n",
    "print(\"class shape=\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(boston.feature_names)\n",
    "print(type(boston.feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some data (Y axis is what we want to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(12,6))\n",
    "ax1.scatter(X[:,11],Y)\n",
    "ax2.scatter(X[:,10],Y)\n",
    "ax3.scatter(X[:,9],Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "sns.set_context(\"poster\")\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "df = pd.DataFrame(X,columns=boston.feature_names)\n",
    "df[\"target\"]  = boston['target']\n",
    "col = cm.viridis(Y/max(Y))\n",
    "g= sns.pairplot(df[['AGE','DIS','RAD','TAX']], \n",
    "                plot_kws=dict(s=80*(Y/max(Y))**3,  color=col, edgecolor=None, alpha=0.7))\n",
    "\n",
    "# make a colorbar\n",
    "cmap = matplotlib.cm.ScalarMappable(\n",
    "      norm = colors.Normalize(min(Y), min(Y)), \n",
    "      cmap = plt.get_cmap('viridis'))\n",
    "\n",
    "cmap.set_array([]) # or alternatively cmap._A = []\n",
    "fig.colorbar(cmap, cax = fig.add_axes([.98, .4, .01, .2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Model Fitting\n",
    "\n",
    "We need to create a **training set** and a **testing set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# half of data\n",
    "import math\n",
    "half = math.floor(len(Y)/2)\n",
    "train_X = X[:half]\n",
    "train_Y = Y[:half]\n",
    "test_X = X[half:]\n",
    "test_Y = Y[half:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Linear Regression **\n",
    "\n",
    "The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the input variables. In mathematical notion, if $\\hat{y}$ is the predicted value.\n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p$$\n",
    "Across the module, we designate the vector $w = (w_1,\n",
    "..., w_p)$ as `coef_` and $w_0$ as `intercept_`.\n",
    "To perform classification with generalized linear models, see Logistic regression.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/linear_model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now do the prediction\n",
    "Y_lr_pred = clf.predict(test_X)\n",
    "\n",
    "# how well did we do?\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(test_Y,Y_lr_pred) ; print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.plot(test_Y,Y_lr_pred - test_Y,'o')\n",
    "ax.set_title(\"Linear Regression Residuals - MSE = %.1f\" % mse)\n",
    "ax.set_xlabel(\"True Median House Price ($1,000)\")\n",
    "ax.set_ylabel(\"Residual\")\n",
    "ax.hlines(0,min(test_Y),max(test_Y),color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *k*-Nearest Neighbor (KNN) Regression **\n",
    "\n",
    "\"The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as non-generalizing machine learning methods, since they simply “remember” all of its training data (possibly transformed into a fast indexing structure such as a Ball Tree or KD Tree.).\"\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_regression_001.png\">\n",
    "\n",
    "http://scikit-learn.org/stable/modules/neighbors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "# many methods work better on scaled X\n",
    "X_scaled = preprocessing.scale(X) \n",
    "clf1 = neighbors.KNeighborsRegressor(5)\n",
    "train_X = X_scaled[:half]\n",
    "test_X = X_scaled[half:]\n",
    "clf1.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_knn_pred = clf1.predict(test_X)\n",
    "mse = mean_squared_error(test_Y,Y_knn_pred) ; print(mse)\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(test_Y, Y_knn_pred - test_Y,'o',alpha=0.4)\n",
    "ax.set_title(\"k-NN Residuals - MSE = %.1f\" % mse)\n",
    "ax.set_xlabel(\"True Median House Price ($1,000)\")\n",
    "ax.set_ylabel(\"Residual\")\n",
    "ax.hlines(0,min(test_Y),max(test_Y),color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what other datasets are readily available..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these we need to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cal_house = datasets.california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cal_data = cal_house.fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cal_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cal_data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = cal_data['data'] # 8 features \n",
    "Y = cal_data['target'] # response (median house price)\n",
    "half = math.floor(len(Y)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# remember: many methods work better on scaled X\n",
    "X_scaled = preprocessing.scale(X) \n",
    "train_X = X_scaled[:half]\n",
    "train_Y = Y[:half]\n",
    "test_X = X_scaled[half:]\n",
    "test_Y = Y[half:]\n",
    "clf1 = neighbors.KNeighborsRegressor(15)\n",
    "clf1.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "Y_knn_pred = clf1.predict(test_X)\n",
    "mse = mean_squared_error(test_Y,Y_knn_pred) ; print(mse)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "ax.scatter(test_Y, Y_knn_pred - test_Y,alpha=0.2,edgecolors=None)\n",
    "ax.set_title(\"k-NN Residuals - MSE = %.1f\" % mse)\n",
    "ax.set_xlabel(\"True log normalized Median House Price\")\n",
    "ax.set_ylabel(\"Residual\")\n",
    "ax.hlines(0,min(test_Y),max(test_Y),color=\"red\")\n",
    "ax.set_xlim(0,5.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Estimation & Model Selection\n",
    "\n",
    "**Q**: How will our model perform on future data?\n",
    "\n",
    "So far, we’ve split the data, using one set to train the model and the other to test its performance\n",
    "\n",
    "This train-test strategy avoids over-fitting to the sample on hand, but wastes data & can produce poor error estimates.\n",
    "\n",
    "cf http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\n",
    "\n",
    "### model selection: cross-validation\n",
    "\n",
    "\n",
    "- *K-fold CV* - randomly split the training data into K folds.  For each $k=1,...,K$, train model only on the data not in fold $k$ & predict for data in fold $k$.  Compute performance metric over CV predictions.\n",
    "\n",
    "- *Leave-one-out (LOO) CV* - K-fold CV with  K = number of training points.\n",
    "\n",
    "<img src=\"https://www.evernote.com/l/AUWvg9caKz1OO7opS2Ji3Z7OwOFkLCrg2WsB/image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i.stack.imgur.com/YWgro.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "boston = datasets.load_boston() ; X = boston['data'] ; Y = boston['target']\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.LinearRegression()\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def print_cv_score_summary(model, xx, yy, cv):\n",
    "    scores = cross_val_score(model, xx, yy, cv=cv, n_jobs=1)\n",
    "    print(\"mean: {:3f}, stdev: {:3f}\".format(\n",
    "        np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_cv_score_summary(clf,X,Y,\n",
    "                       cv=cross_validation.KFold(len(Y), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_cv_score_summary(clf,X,Y,\n",
    "    cv=cross_validation.KFold(len(Y),20,shuffle=True,random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Classification - predicting the discrete class ($y$) of an object from a vector of input features ($\\vec x$).\n",
    "\n",
    "e.g.  $\\vec x_{i=6}  = [5.1,  3.5, 1.4,  0.2]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font size=-1>features = [sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)]</font>\n",
    "\n",
    "and $y_{i=6} =$ “Iris-Setosa” \n",
    "\n",
    "\n",
    "For Iris:  Number of ($\\vec x$, $y$) “instances” = 150\n",
    "\n",
    "Number of classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"feature vector shape=\", X.shape)\n",
    "print(\"class shape=\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(iris.target_names, type(iris.target_names))\n",
    "print(iris.feature_names, type(iris.feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "df = sns.load_dataset(\"iris\")\n",
    "\n",
    "g = sns.PairGrid(df, diag_sharey=False,hue=\"species\")\n",
    "g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "g.map_upper(plt.scatter)\n",
    "g.map_diag(sns.kdeplot, lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - KNN Classification:  `neighbors.KNeighborsClassifier`\n",
    " - LDA / QDA:  `lda.LDA` / `lda.QDA`\n",
    " - Naive Bayes: `naive_bayes.GaussianNB`\n",
    " - Support Vector Machines:   `svm.SVC`\n",
    " - Classification Trees:  `tree.DecisionTreeClassifier`\n",
    " - Random Forest:  `ensemble.RandomForestClassifier`\n",
    " - Multi-class & multi-label Classification is supported: `multiclass.OneVsRest`  `multiclass.OneVsOne` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (1992)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f2/Svm_intro.svg/2000px-Svm_intro.svg.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM with polynomial kernel visualization\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"3liCbRZPrZA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from http://scikit-learn.org/0.13/auto_examples/svm/plot_iris.html#example-svm-plot-iris-py\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 1:3]  # we only take the first two features. We could\n",
    "                      # avoid this ugly slicing by using a two-dim dataset\n",
    "Y = iris.target\n",
    "\n",
    "h = 0.02  # step size in the mesh\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, Y)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)\n",
    "lin_svc = svm.LinearSVC(C=C).fit(X, Y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel',\n",
    "          'LinearSVC (linear kernel)']\n",
    "\n",
    "clfs = [svc, rbf_svc, poly_svc, lin_svc]\n",
    "\n",
    "f,axs = plt.subplots(2,2,figsize=(7,8))\n",
    "\n",
    "for i, clf in enumerate(clfs):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    ax = axs[i//2][i % 2]\n",
    "    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z,cmap=plt.get_cmap(\"viridis\"))\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=Y,cmap=plt.get_cmap(\"viridis\"))\n",
    "\n",
    "    ax.set_title(titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://cs.stanford.edu/people/karpathy/svmjs/demo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNearestNeighbors (kNN)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/2000px-KnnClassification.svg.png\" width=\"50%\">\n",
    "\n",
    "For each test point, $\\vec x_i$ find the $k$-nearest \n",
    "instances in the training data\n",
    "Classify the point according to the majority vote of their \n",
    "class labels.\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/knn.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load http://scikit-learn.org/stable/_downloads/plot_classification.py\n",
    "\"\"\"\n",
    "================================\n",
    "Nearest Neighbors Classification\n",
    "================================\n",
    "\n",
    "Sample usage of Nearest Neighbors classification.\n",
    "It will plot the decision boundaries for each class.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "n_neighbors = 10\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                      # avoid this ugly slicing by using a two-dim dataset\n",
    "y = iris.target\n",
    "\n",
    "h = 0.02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices[:-10]]\n",
    "iris_X_test  = iris_X[indices[-10:]]\n",
    "iris_y_test  = iris_y[indices[-10:]]\n",
    "# Create and fit a nearest-neighbor classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(iris_X_train, iris_y_train) \n",
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
    "           weights='uniform')\n",
    "print(\"predicted:\", knn.predict(iris_X_test))\n",
    "print(\"actual   :\", iris_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model persistence\n",
    "\n",
    "we might sometimes build a model that take a long time to construct. We can easily save the model (on disk) for future use.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "clf = svm.SVC()\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "s    = pickle.dumps(clf)\n",
    "clf2 = pickle.loads(s)\n",
    "print(\"predictions:\",clf2.predict(X)[::15], \" (saved model)\")\n",
    "print(\"true labels:\",y[::15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'filename.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "clf3 = joblib.load('filename.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"predictions:\",clf3.predict(X)[::15], \" (saved model)\")\n",
    "print(\"true labels:\",y[::15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joblib = \"more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
